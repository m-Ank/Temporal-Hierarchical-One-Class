{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_cD8N_EMqNB2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn import preprocessing\n",
        "class DataFactory:\n",
        "    def __init__(self, args, logger):\n",
        "        self.args = args\n",
        "        self.logger = logger\n",
        "        self.home_dir = args.home_dir\n",
        "        self.logger.info(f\"current location: {os.getcwd()}\")\n",
        "        self.logger.info(f\"home dir: {args.home_dir}\")\n",
        "        self.dataset_fn_dict = {\n",
        "            \"NeurIPS-TS-MUL\": self.load_NeurIPS_TS_MUL,\n",
        "            \"SWaT\": self.load_SWaT,\n",
        "            \"SMAP\": self.load_SMAP,\n",
        "            \"MSL\": self.load_MSL,\n",
        "        }\n",
        "        self.datasets = {\n",
        "            \"NeurIPS-TS-MUL\": TSADStandardDataset,\n",
        "            \"SWaT\": TSADStandardDataset,\n",
        "            \"SMAP\": TSADStandardDataset,\n",
        "            \"MSL\": TSADStandardDataset,\n",
        "        }\n",
        "        self.transforms = {\n",
        "            \"minmax\": preprocessing.MinMaxScaler(),\n",
        "            \"std\": preprocessing.StandardScaler(),\n",
        "        }\n",
        "    def __call__(self):\n",
        "        self.logger.info(f\"Preparing {self.args.dataset} ...\")\n",
        "        train_x, train_y, test_x, test_y = self.load()\n",
        "        self.logger.info(\n",
        "            f\"train: X - {train_x.shape}, y - {train_y.shape} \" +\n",
        "            f\"test: X - {test_x.shape}, y - {test_y.shape}\"\n",
        "        )\n",
        "        self.logger.info(f\"Complete.\")\n",
        "        self.logger.info(f\"Preparing dataloader...\")\n",
        "        train_dataset, train_loader, test_dataset, test_loader = self.prepare(\n",
        "            train_x, train_y, test_x, test_y,\n",
        "            window_size=self.args.window_size,\n",
        "            stride=self.args.stride,\n",
        "            dataset_type=self.args.dataset,\n",
        "            batch_size=self.args.batch_size,\n",
        "            eval_batch_size=self.args.eval_batch_size,\n",
        "            train_shuffle=True,\n",
        "            test_shuffle=False,\n",
        "            scaler=self.args.scaler,\n",
        "            window_anomaly=self.args.window_anomaly\n",
        "        )\n",
        "        sample_X, sample_y = next(iter(train_loader))\n",
        "        self.logger.info(f\"total train dataset- {len(train_loader)}, \"\n",
        "                         f\"batch_X - {sample_X.shape}, \"\n",
        "                         f\"batch_y - {sample_y.shape}\")\n",
        "        sample_X, sample_y = next(iter(test_loader))\n",
        "        self.logger.info(f\"total test dataset- {len(test_loader)}, \"\n",
        "                         f\"batch_X - {sample_X.shape}, \"\n",
        "                         f\"batch_y - {sample_y.shape}\")\n",
        "        self.logger.info(f\"Complete.\")\n",
        "        return train_dataset, train_loader, test_dataset, test_loader\n",
        "    def load(self):\n",
        "        return self.dataset_fn_dict[self.args.dataset](self.home_dir)\n",
        "    def prepare(self, train_x, train_y, test_x, test_y,\n",
        "                window_size,\n",
        "                stride,\n",
        "                dataset_type,\n",
        "                batch_size,\n",
        "                eval_batch_size,\n",
        "                train_shuffle,\n",
        "                test_shuffle,\n",
        "                scaler,\n",
        "                window_anomaly,\n",
        "                ):\n",
        "        transform = self.transforms[scaler]\n",
        "        train_dataset = self.datasets[dataset_type](train_x, train_y,\n",
        "                                                    flag=\"train\", transform=transform,\n",
        "                                                    window_size=window_size,\n",
        "                                                    stride=stride,\n",
        "                                                    window_anomaly=window_anomaly,\n",
        "                                                    )\n",
        "        train_dataloader = DataLoader(\n",
        "            dataset=train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=train_shuffle,\n",
        "        )\n",
        "        transform = train_dataset.transform\n",
        "        test_dataset = self.datasets[dataset_type](test_x, test_y,\n",
        "                                                   flag=\"test\", transform=transform,\n",
        "                                                   window_size=window_size,\n",
        "                                                   stride=stride,\n",
        "                                                   window_anomaly=window_anomaly,\n",
        "                                                   )\n",
        "        test_dataloader = DataLoader(\n",
        "            dataset=test_dataset,\n",
        "            batch_size=eval_batch_size,\n",
        "            shuffle=test_shuffle,\n",
        "        )\n",
        "        return train_dataset, train_dataloader, test_dataset, test_dataloader\n",
        "    @staticmethod\n",
        "    def load_NeurIPS_TS_MUL(home_dir=\".\"):\n",
        "        base_dir = \"data/NeurIPS-TS\"\n",
        "        normal = pd.read_csv(os.path.join(home_dir, base_dir, \"nts_mul_normal.csv\"))\n",
        "        abnormal = pd.read_csv(os.path.join(home_dir, base_dir, \"nts_mul_abnormal.csv\"))\n",
        "        train_X, train_y = normal.values[:, :-1], normal.values[:, -1]\n",
        "        test_X, test_y = abnormal.values[:, :-1], abnormal.values[:, -1]\n",
        "        train_X, test_X = train_X.astype(np.float32), test_X.astype(np.float32)\n",
        "        train_y, test_y = train_y.astype(int), test_y.astype(int)\n",
        "        return train_X, train_y, test_X, test_y\n",
        "    @staticmethod\n",
        "    def load_SWaT(home_dir=\".\"):\n",
        "        base_dir = \"data/SWaT\"\n",
        "        SWAT_TRAIN_PATH = os.path.join(home_dir, base_dir, 'SWaT_Dataset_Normal_v0.csv')\n",
        "        SWAT_TEST_PATH = os.path.join(home_dir, base_dir, 'SWaT_Dataset_Attack_v0.csv')\n",
        "        df_train = pd.read_csv(SWAT_TRAIN_PATH, index_col=0)\n",
        "        df_test = pd.read_csv(SWAT_TEST_PATH, index_col=0)\n",
        "        def process_df(df):\n",
        "            for var_index in [item for item in df.columns if item != 'Normal/Attack']:\n",
        "                df[var_index] = pd.to_numeric(df[var_index], errors='coerce')\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "            df.fillna(method='ffill', inplace=True)\n",
        "            x = df.values[:, :-1].astype(np.float32)\n",
        "            y = (df['Normal/Attack'] == 'Attack').to_numpy().astype(int)\n",
        "            return x, y\n",
        "        train_X, train_y = process_df(df_train)\n",
        "        test_X, test_y = process_df(df_test)\n",
        "        train_X, test_X = train_X.astype(np.float32), test_X.astype(np.float32)\n",
        "        train_y, test_y = train_y.astype(int), test_y.astype(int)\n",
        "        return train_X, train_y, test_X, test_y\n",
        "    @staticmethod\n",
        "    def load_SMAP(home_dir=\".\"):\n",
        "        base_dir = \"data/SMAP\"\n",
        "        with open(os.path.join(home_dir, base_dir, \"SMAP_train.pkl\"), 'rb') as f:\n",
        "            train_X = pickle.load(f)\n",
        "        T, C = train_X.shape\n",
        "        train_y = np.zeros((T,), dtype=int)\n",
        "        with open(os.path.join(home_dir, base_dir, \"SMAP_test.pkl\"), 'rb') as f:\n",
        "            test_X = pickle.load(f)\n",
        "        with open(os.path.join(home_dir, base_dir, \"SMAP_test_label.pkl\"), 'rb') as f:\n",
        "            test_y = pickle.load(f)\n",
        "        train_X, test_X = train_X.astype(np.float32), test_X.astype(np.float32)\n",
        "        train_y, test_y = train_y.astype(int), test_y.astype(int)\n",
        "        return train_X, train_y, test_X, test_y\n",
        "    @staticmethod\n",
        "    def load_MSL(home_dir=\".\"):\n",
        "        base_dir = \"data/MSL\"\n",
        "        with open(os.path.join(home_dir, base_dir, \"MSL_train.pkl\"), 'rb') as f:\n",
        "            train_X = pickle.load(f)\n",
        "        T, C = train_X.shape\n",
        "        train_y = np.zeros((T,), dtype=int)\n",
        "        with open(os.path.join(home_dir, base_dir, \"MSL_test.pkl\"), 'rb') as f:\n",
        "            test_X = pickle.load(f)\n",
        "        with open(os.path.join(home_dir, base_dir, \"MSL_test_label.pkl\"), 'rb') as f:\n",
        "            test_y = pickle.load(f)\n",
        "        train_X, test_X = train_X.astype(np.float32), test_X.astype(np.float32)\n",
        "        train_y, test_y = train_y.astype(int), test_y.astype(int)\n",
        "        return train_X, train_y, test_X, test_y\n",
        "class TSADStandardDataset(Dataset):\n",
        "    def __init__(self, x, y, flag, transform, window_size, stride, window_anomaly):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        self.len = (x.shape[0] - window_size) // stride + 1\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "        self.window_anomaly = window_anomaly\n",
        "        x, y = x[:self.len*self.window_size], y[:self.len*self.window_size]\n",
        "        self.x = self.transform.fit_transform(x) if flag == \"train\" else self.transform.transform(x)\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    def __getitem__(self, idx):\n",
        "        _idx = idx * self.stride\n",
        "        label = self.y[_idx:_idx+self.window_size]\n",
        "        X, y = self.x[_idx:_idx+self.window_size], (1 in label) if self.window_anomaly else label\n",
        "        return X, y\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class THOC(nn.Module):\n",
        "    def __init__(self, C, W, n_hidden, tau=1):\n",
        "        self.L = math.floor(math.log(W, 2)) + 1 # number of DRNN layers\n",
        "        self.DRNN = DRNN(n_input=C, n_hidden=n_hidden, n_layers=self.L)\n",
        "        self.clusters = [nn.parameter(n_hidden, K_l) for K_l in range(self.L*2, 0, -2)]\n",
        "        for c in self.clusters:\n",
        "            nn.init.xavier_uniform_(c)\n",
        "        self.tau = 1\n",
        "        self.cnets = [\n",
        "            nn.Sequential(\n",
        "                nn.Linear(n_hidden, n_hidden),\n",
        "                nn.ReLU(),\n",
        "            ) for l in range(self.L) # nn that maps f_bar to f_hat\n",
        "        ]\n",
        "        for n in self.cnets:\n",
        "            nn.init.xavier_uniform_(n)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_hidden*2, n_hidden*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden*2, n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_hidden)\n",
        "        )\n",
        "\n",
        "        self.TSSnets = [\n",
        "            nn.parameter(W-2**l)\n",
        "            for l in range(self.L)\n",
        "        ]\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        :param X: (B, L, C)\n",
        "        :return: Losses,\n",
        "        '''\n",
        "        MTF_output = self.DRNN(X) # Multiscale Temporal Features from dilated RNN.\n",
        "\n",
        "        # THOC\n",
        "        L_THOC = 0\n",
        "        f_t_bar = MTF_output[0][:, -1, :]\n",
        "        Ps, Rs = [], []\n",
        "\n",
        "        for i, c in enumerate(self.clusters):\n",
        "            layer_output = F.cosine_similarity(f_t_bar, c)  # (B, num_cluster_{l-1}, hidden_dim) @ (hidden_dim, num_cluster_{l})\n",
        "            P_ij = F.softmax(layer_output/self.tau, dim=-1) # (B, num_cluster_{l-1}, num_cluster_{l})\n",
        "            R_ij = P_ij if len(Ps)==0 else Rs[i-1] @ P_ij\n",
        "            Ps.append(P_ij)\n",
        "            Rs.append(R_ij)\n",
        "\n",
        "            c_vectors = self.cnets[i](f_t_bar) # (B, num_cluster_{l-1}, hidden_dim)\n",
        "            f_t_bar = P_ij.transpose(-1, -2) @ c_vectors # (B, num_cluster_{l}, hidden_dim)\n",
        "            if i != self.L-1:\n",
        "                f_t_bar = self.MLP(torch.cat((f_t_bar, torch.repeat(MTF_output)[i + 1][:, -1, :].repeat(1, len(c), 1)), dim=-1))\n",
        "\n",
        "            cosine_similarity = (f_t_bar / (torch.norm(f_t_bar, dim=-1) + 1e-08)) @ (c / (torch.norm(f_t_bar, dim=0) + 1e-08))\n",
        "            d = 1 - cosine_similarity\n",
        "            distances = R_ij * d\n",
        "            L_THOC += torch.mean(distances)\n",
        "\n",
        "        # ORTH\n",
        "        L_orth = 0\n",
        "        for c in self.clusters:\n",
        "            c_sq = c.T @ c #(K_l, K_l)\n",
        "            K_l, _ = c_sq.shape\n",
        "            L_orth += torch.linalg.matrix_norm(c_sq-torch.eye(K_l))\n",
        "        L_orth /= len(self.clusters)\n",
        "\n",
        "        # TSS\n",
        "        L_TSS = 0\n",
        "        for i, net in enumerate(self.TSSnets):\n",
        "            src, tgt = X[:, :2**(i), :], X[:, -2**(i):, :]\n",
        "            L_TSS += F.mse_loss(src*net, tgt)\n",
        "\n",
        "        return {\n",
        "            \"L_THOC\": L_THOC,\n",
        "            \"L_orth\": L_orth,\n",
        "            \"L_TSS\": L_TSS\n",
        "        }\n",
        "\n",
        "# Dilated RNN code modified from:\n",
        "# https://github.com/zalandoresearch/pytorch-dilated-rnn/blob/master/drnn.py\n",
        "class DRNN(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden, n_layers, dropout=0, cell_type='GRU', batch_first=True):\n",
        "        super(DRNN, self).__init__()\n",
        "\n",
        "        self.dilations = [2 ** i for i in range(n_layers)]\n",
        "        self.cell_type = cell_type\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        layers = []\n",
        "        if self.cell_type == \"GRU\":\n",
        "            cell = nn.GRU\n",
        "        elif self.cell_type == \"RNN\":\n",
        "            cell = nn.RNN\n",
        "        elif self.cell_type == \"LSTM\":\n",
        "            cell = nn.LSTM\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            if i == 0:\n",
        "                c = cell(n_input, n_hidden, dropout=dropout)\n",
        "            else:\n",
        "                c = cell(n_hidden, n_hidden, dropout=dropout)\n",
        "            layers.append(c)\n",
        "        self.cells = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        inputs = inputs.transpose(0, 1) if self.batch_first else inputs\n",
        "        outputs = []\n",
        "        for i, (cell, dilation) in enumerate(zip(self.cells, self.dilations)):\n",
        "            if hidden is None:\n",
        "                inputs, _ = self.drnn_layer(cell, inputs, dilation)\n",
        "            else:\n",
        "                inputs, hidden[i] = self.drnn_layer(cell, inputs, dilation, hidden[i])\n",
        "            _output = inputs.transpose(0, 1) if self.batch_first else inputs\n",
        "            outputs.append(_output)\n",
        "        return outputs\n",
        "\n",
        "    def drnn_layer(self, cell, inputs, rate, hidden=None):\n",
        "        n_steps = len(inputs)\n",
        "        batch_size = inputs[0].size(0)\n",
        "        hidden_size = cell.hidden_size\n",
        "\n",
        "        inputs, _ = self._pad_inputs(inputs, n_steps, rate)\n",
        "        dilated_inputs = self._prepare_inputs(inputs, rate)\n",
        "\n",
        "        if hidden is None:\n",
        "            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size)\n",
        "        else:\n",
        "            hidden = self._prepare_inputs(hidden, rate)\n",
        "            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size, hidden=hidden)\n",
        "\n",
        "        splitted_outputs = self._split_outputs(dilated_outputs, rate)\n",
        "        outputs = self._unpad_outputs(splitted_outputs, n_steps)\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "    def _apply_cell(self, dilated_inputs, cell, batch_size, rate, hidden_size, hidden=None):\n",
        "        if hidden is None:\n",
        "            if self.cell_type == 'LSTM':\n",
        "                c, m = self.init_hidden(batch_size * rate, hidden_size)\n",
        "                hidden = (c.unsqueeze(0), m.unsqueeze(0))\n",
        "            else:\n",
        "                hidden = self.init_hidden(batch_size * rate, hidden_size).unsqueeze(0)\n",
        "\n",
        "        dilated_outputs, hidden = cell(dilated_inputs, hidden)\n",
        "\n",
        "        return dilated_outputs, hidden\n",
        "\n",
        "    def _unpad_outputs(self, splitted_outputs, n_steps):\n",
        "        return splitted_outputs[:n_steps]\n",
        "\n",
        "    def _split_outputs(self, dilated_outputs, rate):\n",
        "        batchsize = dilated_outputs.size(1) // rate\n",
        "\n",
        "        blocks = [dilated_outputs[:, i * batchsize: (i + 1) * batchsize, :] for i in range(rate)]\n",
        "\n",
        "        interleaved = torch.stack((blocks)).transpose(1, 0).contiguous()\n",
        "        interleaved = interleaved.view(dilated_outputs.size(0) * rate,\n",
        "                                       batchsize,\n",
        "                                       dilated_outputs.size(2))\n",
        "        return interleaved\n",
        "\n",
        "    def _pad_inputs(self, inputs, n_steps, rate):\n",
        "        is_even = (n_steps % rate) == 0\n",
        "\n",
        "        if not is_even:\n",
        "            dilated_steps = n_steps // rate + 1\n",
        "\n",
        "            zeros_ = torch.zeros(dilated_steps * rate - inputs.size(0),\n",
        "                                 inputs.size(1),\n",
        "                                 inputs.size(2))\n",
        "\n",
        "            inputs = torch.cat((inputs, zeros_))\n",
        "        else:\n",
        "            dilated_steps = n_steps // rate\n",
        "\n",
        "        return inputs, dilated_steps\n",
        "\n",
        "    def _prepare_inputs(self, inputs, rate):\n",
        "        dilated_inputs = torch.cat([inputs[j::rate, :, :] for j in range(rate)], 1)\n",
        "        return dilated_inputs\n",
        "\n",
        "    def init_hidden(self, batch_size, hidden_dim):\n",
        "        hidden = torch.zeros(batch_size, hidden_dim)\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            memory = torch.zeros(batch_size, hidden_dim)\n",
        "            return (hidden, memory)\n",
        "        else:\n",
        "            return hidden\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    B, L, C = 64, 12, 51\n",
        "    ip = torch.randn((B, L, C))\n",
        "    drnn = DRNN(n_input=C, n_hidden=128, n_layers=3, dropout=0, cell_type='GRU', batch_first=True)\n",
        "\n",
        "    print(drnn)\n",
        "    print(drnn(ip))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNtHl1UUqdJ5",
        "outputId": "9dad1aaa-ccf7-46db-ba06-612b41231c04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DRNN(\n",
            "  (cells): Sequential(\n",
            "    (0): GRU(51, 128)\n",
            "    (1): GRU(128, 128)\n",
            "    (2): GRU(128, 128)\n",
            "  )\n",
            ")\n",
            "[tensor([[[ 2.4335e-01,  7.4354e-02,  2.6042e-01,  ..., -1.2665e-01,\n",
            "           3.9342e-01, -1.0363e-01],\n",
            "         [ 1.8304e-01, -5.1456e-02,  1.8631e-01,  ..., -6.3175e-02,\n",
            "           3.6321e-01, -2.2124e-01],\n",
            "         [ 2.8701e-01,  2.1541e-01,  3.3847e-01,  ..., -7.1447e-02,\n",
            "           4.7187e-02,  8.9042e-02],\n",
            "         ...,\n",
            "         [-9.5338e-02,  1.7424e-01, -7.5130e-04,  ...,  1.2272e-02,\n",
            "          -2.2933e-01,  1.8912e-01],\n",
            "         [-1.0974e-01,  7.3719e-02, -6.6230e-02,  ...,  1.2789e-01,\n",
            "          -4.0509e-01,  3.7038e-02],\n",
            "         [ 1.9439e-01,  1.1886e-01,  7.9913e-02,  ...,  2.3062e-01,\n",
            "          -1.5337e-01, -3.3471e-01]],\n",
            "\n",
            "        [[-1.2886e-01, -9.7856e-02,  6.8023e-02,  ...,  5.1745e-02,\n",
            "           1.5321e-02, -6.5128e-02],\n",
            "         [-2.3255e-02,  1.6872e-01,  6.4116e-02,  ..., -4.9134e-02,\n",
            "          -8.4108e-02,  8.8218e-02],\n",
            "         [ 1.5735e-01,  2.9138e-01,  1.6989e-01,  ..., -1.3499e-01,\n",
            "           1.5681e-01, -5.1135e-02],\n",
            "         ...,\n",
            "         [-9.8013e-02, -3.1278e-02,  1.5492e-01,  ..., -2.8406e-01,\n",
            "           3.2978e-01, -1.0278e-01],\n",
            "         [-1.4806e-01, -1.1400e-01, -1.4408e-01,  ..., -1.9613e-01,\n",
            "           5.0579e-01,  3.2558e-02],\n",
            "         [ 8.1747e-02,  2.2525e-01,  2.1211e-01,  ...,  1.0160e-01,\n",
            "           3.9837e-01, -1.9067e-01]],\n",
            "\n",
            "        [[-1.4322e-01,  1.7800e-01, -7.7350e-02,  ..., -2.0130e-02,\n",
            "           1.9081e-01,  3.1201e-02],\n",
            "         [ 6.7586e-02,  1.6839e-01,  2.2434e-01,  ..., -1.9151e-01,\n",
            "          -3.2911e-02, -1.3042e-01],\n",
            "         [ 1.0455e-01, -2.0027e-01, -6.7784e-02,  ..., -2.4888e-01,\n",
            "           9.6179e-02,  6.7931e-02],\n",
            "         ...,\n",
            "         [-1.3816e-01, -1.3419e-01,  2.3026e-01,  ..., -2.6907e-02,\n",
            "           8.3203e-02,  2.9743e-02],\n",
            "         [ 4.8463e-02, -2.2776e-02,  2.2458e-01,  ..., -4.0677e-02,\n",
            "          -1.7481e-01, -3.0261e-01],\n",
            "         [ 2.3992e-01,  1.7942e-01,  5.9554e-02,  ..., -1.1421e-01,\n",
            "           3.3300e-02, -1.9409e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 5.3437e-02, -1.9371e-01, -7.9662e-02,  ..., -1.9571e-01,\n",
            "          -5.2516e-02, -1.0710e-01],\n",
            "         [ 1.0090e-01, -6.8832e-02,  7.8789e-02,  ..., -1.9139e-01,\n",
            "          -1.3971e-01,  5.1643e-02],\n",
            "         [ 2.6854e-01,  9.8750e-02,  1.0189e-01,  ..., -2.9843e-01,\n",
            "          -9.2201e-02,  1.5126e-01],\n",
            "         ...,\n",
            "         [ 3.5675e-01,  1.3588e-01, -1.0021e-01,  ..., -5.0523e-02,\n",
            "           4.2338e-01, -1.0316e-01],\n",
            "         [ 3.0703e-01,  1.3465e-01,  1.2918e-01,  ..., -5.1854e-02,\n",
            "           1.8426e-01, -8.1794e-02],\n",
            "         [ 1.8776e-01,  2.7039e-01,  2.1364e-01,  ...,  2.4251e-01,\n",
            "           9.5981e-02,  2.7949e-02]],\n",
            "\n",
            "        [[ 2.6473e-02,  1.1373e-02,  2.9327e-01,  ...,  1.6528e-01,\n",
            "           3.6571e-01,  3.7373e-01],\n",
            "         [ 1.6904e-01,  5.1371e-02,  1.9526e-01,  ..., -1.3761e-01,\n",
            "           2.3125e-01,  6.6906e-02],\n",
            "         [ 1.0857e-01,  1.9132e-01, -1.8439e-01,  ..., -5.2897e-02,\n",
            "           2.3342e-02,  2.5819e-02],\n",
            "         ...,\n",
            "         [ 3.1715e-04,  1.9163e-01,  2.9899e-01,  ..., -1.7887e-01,\n",
            "          -3.1794e-01,  3.2172e-01],\n",
            "         [-5.6263e-02,  1.4711e-01,  4.5036e-01,  ..., -2.7616e-01,\n",
            "          -3.3685e-01,  1.9845e-01],\n",
            "         [ 1.5407e-01, -8.1140e-02,  1.8611e-01,  ..., -2.1404e-01,\n",
            "          -9.4788e-02, -2.2773e-01]],\n",
            "\n",
            "        [[ 2.9998e-01, -8.5457e-02,  2.4165e-01,  ..., -1.8107e-01,\n",
            "           6.1048e-02,  2.3832e-01],\n",
            "         [ 4.1398e-01, -6.1510e-02,  2.0299e-01,  ..., -4.9844e-02,\n",
            "           1.8314e-01, -2.0058e-01],\n",
            "         [ 1.7981e-01,  2.8696e-01, -3.3698e-01,  ...,  7.0264e-03,\n",
            "           1.6136e-01, -1.7327e-02],\n",
            "         ...,\n",
            "         [ 2.8485e-01, -5.6053e-02, -8.8449e-02,  ...,  2.4963e-02,\n",
            "           2.6236e-01, -8.5720e-02],\n",
            "         [ 3.7857e-01,  3.2077e-01, -7.0368e-02,  ...,  9.7243e-02,\n",
            "           3.9850e-01,  1.7559e-01],\n",
            "         [ 3.1635e-01,  4.4074e-02, -1.0590e-01,  ...,  1.6197e-02,\n",
            "           4.4925e-01,  2.5241e-01]]], grad_fn=<TransposeBackward0>), tensor([[[ 0.0532, -0.0823, -0.0184,  ..., -0.0042, -0.0159,  0.0705],\n",
            "         [ 0.0545, -0.0156, -0.0803,  ...,  0.0104, -0.1313,  0.0350],\n",
            "         [ 0.1074, -0.1928, -0.0294,  ...,  0.0487, -0.1149,  0.0660],\n",
            "         ...,\n",
            "         [ 0.1030, -0.1957, -0.0395,  ...,  0.0339, -0.0655, -0.0968],\n",
            "         [ 0.0899, -0.1034, -0.0345,  ...,  0.0205, -0.0812, -0.0205],\n",
            "         [ 0.0811, -0.1421, -0.0915,  ...,  0.0080, -0.1881, -0.0378]],\n",
            "\n",
            "        [[ 0.0662, -0.0513, -0.0803,  ..., -0.0489, -0.0374,  0.0317],\n",
            "         [ 0.1201, -0.1589, -0.0116,  ..., -0.0189, -0.0461, -0.0423],\n",
            "         [ 0.0377, -0.0483,  0.0368,  ..., -0.0612, -0.0784, -0.0148],\n",
            "         ...,\n",
            "         [ 0.1245, -0.1799, -0.0905,  ...,  0.0340, -0.0590, -0.1902],\n",
            "         [ 0.0919, -0.0687, -0.0730,  ...,  0.0304, -0.0193, -0.1415],\n",
            "         [ 0.0994, -0.1696, -0.0880,  ...,  0.0374, -0.0639, -0.1158]],\n",
            "\n",
            "        [[ 0.0922, -0.1055, -0.0565,  ...,  0.0164, -0.0935,  0.0340],\n",
            "         [-0.0497, -0.1689, -0.0353,  ...,  0.0247, -0.1619,  0.0022],\n",
            "         [-0.0341, -0.1175, -0.1083,  ..., -0.0258, -0.1131,  0.0090],\n",
            "         ...,\n",
            "         [-0.0382, -0.1165, -0.1390,  ..., -0.0125, -0.0892, -0.0181],\n",
            "         [-0.0385, -0.0817, -0.0932,  ..., -0.0865, -0.1183, -0.0428],\n",
            "         [ 0.0641, -0.0745, -0.0572,  ..., -0.0248, -0.0534,  0.0025]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0282, -0.0509, -0.0993,  ..., -0.0262, -0.1109,  0.0691],\n",
            "         [-0.0493,  0.0380, -0.0931,  ..., -0.0191, -0.1283,  0.0157],\n",
            "         [-0.0194, -0.0222, -0.1806,  ...,  0.0469, -0.1818,  0.0878],\n",
            "         ...,\n",
            "         [ 0.1244, -0.0822, -0.0574,  ...,  0.0625, -0.1773, -0.0072],\n",
            "         [ 0.0086, -0.1371, -0.0401,  ...,  0.0720, -0.0305,  0.0433],\n",
            "         [ 0.0331, -0.1064,  0.0034,  ...,  0.1141, -0.1326, -0.0386]],\n",
            "\n",
            "        [[ 0.0407, -0.0509, -0.0953,  ..., -0.0301, -0.0536,  0.0948],\n",
            "         [ 0.0475, -0.0562, -0.0050,  ...,  0.0171, -0.0676, -0.0244],\n",
            "         [ 0.0648, -0.0476, -0.0137,  ...,  0.0037, -0.1740,  0.0510],\n",
            "         ...,\n",
            "         [ 0.0109, -0.0387, -0.0175,  ...,  0.0918, -0.0411,  0.0293],\n",
            "         [ 0.0295, -0.0018, -0.1084,  ..., -0.0087, -0.1551,  0.0335],\n",
            "         [ 0.1139, -0.0271, -0.1223,  ...,  0.0576, -0.2143,  0.0187]],\n",
            "\n",
            "        [[ 0.0104, -0.0361, -0.0404,  ..., -0.0474, -0.0606,  0.0771],\n",
            "         [ 0.0595,  0.0162, -0.0662,  ...,  0.0022,  0.0049, -0.0030],\n",
            "         [ 0.0861,  0.0166,  0.0145,  ..., -0.0273, -0.0605,  0.0756],\n",
            "         ...,\n",
            "         [ 0.0251, -0.1562, -0.0481,  ...,  0.0310, -0.0831,  0.1601],\n",
            "         [ 0.0385, -0.0674, -0.0181,  ...,  0.1024, -0.0824,  0.1070],\n",
            "         [-0.0217, -0.0280, -0.0413,  ...,  0.0448, -0.1100,  0.1529]]],\n",
            "       grad_fn=<TransposeBackward0>), tensor([[[ 0.0051,  0.0429, -0.0380,  ...,  0.0033,  0.0022,  0.0154],\n",
            "         [-0.0224,  0.0571, -0.0221,  ..., -0.0265,  0.0104,  0.0138],\n",
            "         [-0.0229,  0.0415, -0.0506,  ..., -0.0184,  0.0065,  0.0100],\n",
            "         ...,\n",
            "         [-0.0383,  0.1137, -0.0296,  ...,  0.0166, -0.0168,  0.0087],\n",
            "         [-0.0172,  0.1124, -0.0210,  ...,  0.0122, -0.0190, -0.0141],\n",
            "         [-0.0342,  0.1109, -0.0100,  ...,  0.0067, -0.0257, -0.0176]],\n",
            "\n",
            "        [[-0.0117,  0.0603, -0.0110,  ..., -0.0064, -0.0135,  0.0219],\n",
            "         [-0.0161,  0.0736, -0.0123,  ..., -0.0210,  0.0148, -0.0004],\n",
            "         [-0.0252,  0.0682, -0.0099,  ..., -0.0070, -0.0355, -0.0015],\n",
            "         ...,\n",
            "         [-0.0445,  0.1724, -0.0273,  ..., -0.0476, -0.0565, -0.0002],\n",
            "         [-0.0301,  0.1605, -0.0354,  ..., -0.0280, -0.0569, -0.0028],\n",
            "         [-0.0453,  0.1950, -0.0538,  ..., -0.0416, -0.0305,  0.0030]],\n",
            "\n",
            "        [[-0.0255,  0.0517, -0.0269,  ..., -0.0233, -0.0083,  0.0172],\n",
            "         [ 0.0093,  0.0803, -0.0098,  ..., -0.0174, -0.0402,  0.0206],\n",
            "         [-0.0041,  0.0832,  0.0123,  ...,  0.0071, -0.0014,  0.0133],\n",
            "         ...,\n",
            "         [-0.0661,  0.1726,  0.0176,  ..., -0.0429, -0.0214,  0.0080],\n",
            "         [-0.0264,  0.1472,  0.0112,  ..., -0.0280,  0.0037,  0.0108],\n",
            "         [-0.0388,  0.1522, -0.0218,  ..., -0.0154, -0.0043,  0.0120]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0016,  0.0454, -0.0090,  ..., -0.0058, -0.0272,  0.0105],\n",
            "         [ 0.0092,  0.0448,  0.0163,  ..., -0.0094, -0.0201,  0.0211],\n",
            "         [-0.0142,  0.0256, -0.0112,  ...,  0.0095, -0.0288,  0.0136],\n",
            "         ...,\n",
            "         [-0.0902,  0.0969, -0.0258,  ...,  0.0127, -0.0426,  0.0182],\n",
            "         [-0.0675,  0.1119, -0.0391,  ...,  0.0169, -0.0466,  0.0067],\n",
            "         [-0.0789,  0.1165, -0.0274,  ...,  0.0179, -0.0491,  0.0208]],\n",
            "\n",
            "        [[-0.0425,  0.0671, -0.0287,  ..., -0.0185, -0.0103,  0.0220],\n",
            "         [-0.0414,  0.0460, -0.0121,  ..., -0.0216, -0.0056,  0.0069],\n",
            "         [-0.0068,  0.0713, -0.0094,  ..., -0.0180, -0.0107,  0.0262],\n",
            "         ...,\n",
            "         [-0.0511,  0.0543,  0.0244,  ..., -0.0026, -0.0532, -0.0244],\n",
            "         [ 0.0006,  0.1303,  0.0224,  ..., -0.0273, -0.0342,  0.0257],\n",
            "         [-0.0461,  0.0864,  0.0047,  ..., -0.0228, -0.0645, -0.0166]],\n",
            "\n",
            "        [[-0.0150,  0.0485, -0.0245,  ..., -0.0358,  0.0115,  0.0273],\n",
            "         [-0.0197,  0.0451, -0.0242,  ..., -0.0217, -0.0022,  0.0235],\n",
            "         [-0.0250,  0.0361, -0.0212,  ..., -0.0223,  0.0030, -0.0085],\n",
            "         ...,\n",
            "         [-0.0980,  0.0424,  0.0251,  ...,  0.0225, -0.0212, -0.0310],\n",
            "         [-0.0821,  0.0471, -0.0255,  ...,  0.0276, -0.0340, -0.0245],\n",
            "         [-0.0798,  0.0229,  0.0465,  ...,  0.0243, -0.0429, -0.0614]]],\n",
            "       grad_fn=<TransposeBackward0>)]\n"
          ]
        }
      ]
    }
  ]
}